#!/usr/bin/env python3
"""
Agent B: Outline Builder

–í–•–û–î:
  sources/<book_id>/raw_text.md    - –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∫–Ω–∏–≥–∏
  sources/<book_id>/metadata.json  - –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞

–í–´–•–û–î:
  work/<methodology_id>/outline.yaml - –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏
  work/<methodology_id>/sections.json - –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Å–µ–∫—Ü–∏–∏
  work/<methodology_id>/metadata.json - –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏

–ó–ê–î–ê–ß–ò:
  1. –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Ç–∏–ø –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ (diagnostic, planning, analysis)
  2. –ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–µ–∫—Ü–∏–∏
  3. –ù–∞–π—Ç–∏ indicators, rules, concepts
  4. –°–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å —Å glossary
  5. –°–æ–∑–¥–∞—Ç—å outline.yaml –¥–ª—è Agent C
"""

import sys
import json
import yaml
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
from dataclasses import dataclass, asdict


@dataclass
class Section:
    """–°–µ–∫—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    title: str
    content: str
    start_line: int
    end_line: int
    level: int  # 1=chapter, 2=section, 3=subsection
    type: str  # 'concept', 'tool', 'indicator', 'rule', 'example'


@dataclass
class Indicator:
    """–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä"""
    name: str
    formula: Optional[str]
    unit: Optional[str]
    context: str  # –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∫–Ω–∏–≥–∏
    glossary_match: Optional[str]  # –ù–∞–π–¥–µ–Ω–Ω—ã–π —Ç–µ—Ä–º–∏–Ω –∏–∑ glossary


@dataclass
class Rule:
    """–ü—Ä–∞–≤–∏–ª–æ/—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è"""
    description: str
    condition: Optional[str]
    action: str
    context: str
    priority: str  # 'high', 'medium', 'low'


@dataclass
class Outline:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏"""
    methodology_id: str
    title: str
    category: str  # cash_flow, profitability, working_capital, etc
    level: str  # strategic, tactical, operational
    
    source_book: str
    extraction_date: str
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    overview: str
    key_concepts: List[str]
    indicators: List[Dict]
    rules: List[Dict]
    stages: List[Dict]
    
    # –°–≤—è–∑–∏ —Å glossary
    glossary_matches: Dict[str, str]
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    confidence: float  # 0-1, –Ω–∞—Å–∫–æ–ª—å–∫–æ —É–≤–µ—Ä–µ–Ω—ã –≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    notes: List[str]


class OutlineBuilder:
    """–°—Ç—Ä–æ–∏—Ç–µ–ª—å —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏"""
    
    def __init__(self, glossary_dir: Path = Path('data/glossary')):
        self.glossary_dir = glossary_dir
        self.glossary_terms = self._load_glossary()
    
    def _load_glossary(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ glossary"""
        terms = {}
        
        if not self.glossary_dir.exists():
            print(f"‚ö†Ô∏è Glossary directory not found: {self.glossary_dir}")
            return terms
        
        for yaml_file in self.glossary_dir.glob('*.yaml'):
            try:
                with open(yaml_file, 'r', encoding='utf-8') as f:
                    term_data = yaml.safe_load(f)
                    term_id = yaml_file.stem
                    terms[term_id] = term_data
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to load {yaml_file}: {e}")
        
        print(f"‚úÖ Loaded {len(terms)} glossary terms")
        return terms
    
    def build(self, book_id: str, methodology_id: str) -> Outline:
        """
        –ü–æ—Å—Ç—Ä–æ–∏—Ç—å outline –∏–∑ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        
        Args:
            book_id: ID –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π –∫–Ω–∏–≥–∏ (–∏–∑ sources/)
            methodology_id: ID —Å–æ–∑–¥–∞–≤–∞–µ–º–æ–π –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏
        
        Returns:
            Outline —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏
        """
        
        # –ß–∏—Ç–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        sources_dir = Path('sources') / book_id
        raw_text_file = sources_dir / 'raw_text.md'
        metadata_file = sources_dir / 'metadata.json'
        
        if not raw_text_file.exists():
            raise FileNotFoundError(f"Raw text not found: {raw_text_file}")
        
        text = raw_text_file.read_text(encoding='utf-8')
        metadata = json.loads(metadata_file.read_text()) if metadata_file.exists() else {}
        
        print(f"üìñ Processing: {book_id}")
        print(f"   Text length: {len(text)} chars")
        print(f"   Lines: {len(text.splitlines())}")
        
        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–µ–∫—Ü–∏–∏
        sections = self._extract_sections(text)
        print(f"   Sections: {len(sections)}")
        
        # 2. –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é
        category, level = self._classify_methodology(text, sections)
        print(f"   Category: {category}, Level: {level}")
        
        # 3. –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        indicators = self._extract_indicators(text, sections)
        print(f"   Indicators: {len(indicators)}")
        
        # 4. –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∞–≤–∏–ª–∞
        rules = self._extract_rules(text, sections)
        print(f"   Rules: {len(rules)}")
        
        # 5. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
        key_concepts = self._extract_concepts(text, sections)
        print(f"   Concepts: {len(key_concepts)}")
        
        # 6. –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º —Å glossary
        glossary_matches = self._match_glossary(text, indicators, key_concepts)
        print(f"   Glossary matches: {len(glossary_matches)}")
        
        # 7. –ò–∑–≤–ª–µ–∫–∞–µ–º —ç—Ç–∞–ø—ã (stages)
        stages = self._extract_stages(sections)
        print(f"   Stages: {len(stages)}")
        
        # 8. –°–æ–∑–¥–∞—ë–º overview
        overview = self._generate_overview(text, sections)
        
        # –°–æ–±–∏—Ä–∞–µ–º outline
        outline = Outline(
            methodology_id=methodology_id,
            title=self._extract_title(text, metadata),
            category=category,
            level=level,
            source_book=book_id,
            extraction_date=datetime.now().isoformat(),
            overview=overview,
            key_concepts=key_concepts,
            indicators=[asdict(ind) for ind in indicators],
            rules=[asdict(rule) for rule in rules],
            stages=stages,
            glossary_matches=glossary_matches,
            confidence=0.8,  # TODO: –≤—ã—á–∏—Å–ª—è—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
            notes=[
                "Auto-generated by Agent B",
                f"Source: {metadata.get('source_file', 'unknown')}",
                f"Method: {metadata.get('method', 'unknown')}"
            ]
        )
        
        return outline
    
    def _extract_sections(self, text: str) -> List[Section]:
        """–ò–∑–≤–ª–µ—á—å —Å–µ–∫—Ü–∏–∏ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º"""
        sections = []
        lines = text.splitlines()
        
        current_section = None
        current_content = []
        
        for i, line in enumerate(lines, 1):
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –∑–∞–≥–æ–ª–æ–≤–∫–∞
            level = None
            title = None
            
            if line.startswith('# '):
                level, title = 1, line[2:].strip()
            elif line.startswith('## '):
                level, title = 2, line[3:].strip()
            elif line.startswith('### '):
                level, title = 3, line[4:].strip()
            
            if level:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å–µ–∫—Ü–∏—é
                if current_section:
                    current_section.content = '\n'.join(current_content)
                    current_section.end_line = i - 1
                    sections.append(current_section)
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é —Å–µ–∫—Ü–∏—é
                current_section = Section(
                    title=title,
                    content='',
                    start_line=i,
                    end_line=i,
                    level=level,
                    type='unknown'
                )
                current_content = []
            elif current_section:
                current_content.append(line)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å–µ–∫—Ü–∏—é
        if current_section:
            current_section.content = '\n'.join(current_content)
            current_section.end_line = len(lines)
            sections.append(current_section)
        
        return sections
    
    def _classify_methodology(self, text: str, sections: List[Section]) -> tuple:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é"""
        
        text_lower = text.lower()
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏—è
        categories = {
            'cash_flow': ['–¥–µ–Ω–µ–∂–Ω—ã–π –ø–æ—Ç–æ–∫', 'cash flow', '–æ–±–æ—Ä–æ—Ç–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª'],
            'profitability': ['–ø—Ä–∏–±—ã–ª—å', '—Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å', 'profit', 'margin'],
            'working_capital': ['–æ–±–æ—Ä–æ—Ç–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª', 'working capital', '–∑–∞–ø–∞—Å—ã'],
            'costs': ['–∑–∞—Ç—Ä–∞—Ç—ã', '—Ä–∞—Å—Ö–æ–¥—ã', 'costs', 'expenses'],
            'pricing': ['—Ü–µ–Ω–∞', 'pricing', '—Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ'],
        }
        
        category_scores = {}
        for cat, keywords in categories.items():
            score = sum(text_lower.count(kw) for kw in keywords)
            category_scores[cat] = score
        
        category = max(category_scores, key=category_scores.get)
        
        # –£—Ä–æ–≤–µ–Ω—å
        if any(kw in text_lower for kw in ['—Å—Ç—Ä–∞—Ç–µ–≥–∏—è', 'strategy', '–¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω']):
            level = 'strategic'
        elif any(kw in text_lower for kw in ['–º–µ—Ç—Ä–∏–∫–∞', 'indicator', 'kpi', '–ø–æ–∫–∞–∑–∞—Ç–µ–ª—å']):
            level = 'tactical'
        else:
            level = 'operational'
        
        return category, level
    
    def _extract_indicators(self, text: str, sections: List[Section]) -> List[Indicator]:
        """–ò–∑–≤–ª–µ—á—å —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã"""
        indicators = []
        
        # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ç—Ç–µ—Ä–Ω: –∏—â–µ–º —Ñ–æ—Ä–º—É–ª—ã –∏ —á–∏—Å–ª–æ–≤—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
        lines = text.splitlines()
        for i, line in enumerate(lines):
            # –ò—â–µ–º —Ñ–æ—Ä–º—É–ª—ã: "ROI = ..."
            if '=' in line and any(c.isupper() for c in line):
                parts = line.split('=', 1)
                if len(parts) == 2:
                    name = parts[0].strip()
                    formula = parts[1].strip()
                    
                    # –ò—â–µ–º –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è
                    unit = None
                    if '%' in formula:
                        unit = 'percent'
                    elif '—Ä—É–±' in formula.lower() or 'rub' in formula.lower():
                        unit = 'rub'
                    
                    # –ö–æ–Ω—Ç–µ–∫—Å—Ç (3 —Å—Ç—Ä–æ–∫–∏ –¥–æ –∏ –ø–æ—Å–ª–µ)
                    context_lines = lines[max(0, i-3):min(len(lines), i+4)]
                    context = '\n'.join(context_lines)
                    
                    # –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å glossary
                    glossary_match = self._find_glossary_match(name)
                    
                    indicators.append(Indicator(
                        name=name,
                        formula=formula if len(formula) < 200 else formula[:200] + '...',
                        unit=unit,
                        context=context[:300],
                        glossary_match=glossary_match
                    ))
        
        return indicators[:50]  # –ú–∞–∫—Å–∏–º—É–º 50 –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
    
    def _extract_rules(self, text: str, sections: List[Section]) -> List[Rule]:
        """–ò–∑–≤–ª–µ—á—å –ø—Ä–∞–≤–∏–ª–∞ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"""
        rules = []
        
        # –ò—â–µ–º –∏–º–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        rule_keywords = [
            '–¥–æ–ª–∂–µ–Ω', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ', '—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è', '—Å–ª–µ–¥—É–µ—Ç',
            '–≤–∞–∂–Ω–æ', '–∫—Ä–∏—Ç–∏—á–Ω–æ', '–Ω—É–∂–Ω–æ', '—Ç—Ä–µ–±—É–µ—Ç—Å—è'
        ]
        
        lines = text.splitlines()
        for i, line in enumerate(lines):
            if any(kw in line.lower() for kw in rule_keywords):
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                priority = 'high' if any(w in line.lower() for w in ['–∫—Ä–∏—Ç–∏—á–Ω–æ', '–≤–∞–∂–Ω–æ', '–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ']) else 'medium'
                
                # –ö–æ–Ω—Ç–µ–∫—Å—Ç
                context_lines = lines[max(0, i-2):min(len(lines), i+3)]
                context = '\n'.join(context_lines)
                
                rules.append(Rule(
                    description=line.strip(),
                    condition=None,  # TODO: –∏–∑–≤–ª–µ–∫–∞—Ç—å —É—Å–ª–æ–≤–∏—è
                    action=line.strip(),
                    context=context[:200],
                    priority=priority
                ))
        
        return rules[:30]  # –ú–∞–∫—Å–∏–º—É–º 30 –ø—Ä–∞–≤–∏–ª
    
    def _extract_concepts(self, text: str, sections: List[Section]) -> List[str]:
        """–ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏"""
        concepts = []
        
        # –ò—â–µ–º —Ç–µ—Ä–º–∏–Ω—ã –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö —Å–µ–∫—Ü–∏–π
        for section in sections:
            if section.level <= 2:  # –¢–æ–ª—å–∫–æ –≥–ª–∞–≤–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                concepts.append(section.title)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ glossary, –µ—Å–ª–∏ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è
        for term_id, term_data in self.glossary_terms.items():
            term_title = term_data.get('title', term_id)
            if term_title.lower() in text.lower():
                concepts.append(term_title)
        
        return list(set(concepts))[:20]  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ, –º–∞–∫—Å–∏–º—É–º 20
    
    def _match_glossary(self, text: str, indicators: List[Indicator], 
                       concepts: List[str]) -> Dict[str, str]:
        """–°–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å —Å glossary"""
        matches = {}
        
        text_lower = text.lower()
        
        for term_id, term_data in self.glossary_terms.items():
            term_title = term_data.get('title', term_id).lower()
            
            if term_title in text_lower:
                matches[term_id] = term_data.get('title', term_id)
        
        return matches
    
    def _find_glossary_match(self, name: str) -> Optional[str]:
        """–ù–∞–π—Ç–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –≤ glossary"""
        name_lower = name.lower()
        
        for term_id, term_data in self.glossary_terms.items():
            term_title = term_data.get('title', '').lower()
            if name_lower in term_title or term_title in name_lower:
                return term_id
        
        return None
    
    def _extract_stages(self, sections: List[Section]) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á—å —ç—Ç–∞–ø—ã –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏"""
        stages = []
        
        # –ò—â–µ–º —Å–µ–∫—Ü–∏–∏ —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é (—à–∞–≥ 1, —ç—Ç–∞–ø 1, etc)
        stage_keywords = ['—à–∞–≥', '—ç—Ç–∞–ø', 'step', 'stage', '—Ñ–∞–∑–∞']
        
        for section in sections:
            if any(kw in section.title.lower() for kw in stage_keywords):
                stages.append({
                    'title': section.title,
                    'description': section.content[:200] + '...' if len(section.content) > 200 else section.content,
                    'order': len(stages) + 1
                })
        
        return stages
    
    def _generate_overview(self, text: str, sections: List[Section]) -> str:
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ"""
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é —Å–µ–∫—Ü–∏—é –∏–ª–∏ –ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤
        if sections:
            return sections[0].content[:500] + '...'
        else:
            return text[:500] + '...'
    
    def _extract_title(self, text: str, metadata: Dict) -> str:
        """–ò–∑–≤–ª–µ—á—å –Ω–∞–∑–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏"""
        lines = text.splitlines()
        
        # –ò—â–µ–º –ø–µ—Ä–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
        for line in lines[:20]:
            if line.startswith('# '):
                return line[2:].strip()
        
        # Fallback: –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        return metadata.get('source_file', 'Unknown').split('/')[-1]
    
    def save(self, outline: Outline, output_dir: Path):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å outline"""
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º outline.yaml
        outline_file = output_dir / 'outline.yaml'
        outline_dict = asdict(outline)
        
        with open(outline_file, 'w', encoding='utf-8') as f:
            yaml.dump(outline_dict, f, allow_unicode=True, sort_keys=False)
        
        print(f"‚úÖ Saved outline: {outline_file}")
        
        # 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º metadata.json
        metadata_file = output_dir / 'metadata.json'
        metadata = {
            'methodology_id': outline.methodology_id,
            'source_book': outline.source_book,
            'extraction_date': outline.extraction_date,
            'confidence': outline.confidence,
            'agent': 'Agent B: Outline Builder',
        }
        
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ Saved metadata: {metadata_file}")
        
        return {
            'outline_file': str(outline_file),
            'metadata_file': str(metadata_file),
        }


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Agent B: Build methodology outline from extracted text'
    )
    parser.add_argument('book_id', type=str, help='Book ID (from sources/)')
    parser.add_argument('--methodology-id', type=str, required=True,
                       help='Methodology ID to create')
    parser.add_argument('--output-dir', type=Path, default=None,
                       help='Output directory (default: work/<methodology_id>/)')
    
    args = parser.parse_args()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º output_dir
    if args.output_dir is None:
        args.output_dir = Path('work') / args.methodology_id
    
    try:
        builder = OutlineBuilder()
        outline = builder.build(args.book_id, args.methodology_id)
        
        result = builder.save(outline, args.output_dir)
        
        print(f"\n‚úÖ Agent B completed!")
        print(f"   Methodology: {outline.methodology_id}")
        print(f"   Category: {outline.category}")
        print(f"   Indicators: {len(outline.indicators)}")
        print(f"   Rules: {len(outline.rules)}")
        print(f"   Concepts: {len(outline.key_concepts)}")
        print(f"   Glossary matches: {len(outline.glossary_matches)}")
    
    except Exception as e:
        print(f"‚ùå Error: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
