"""
Agent B: Outline Builder
–ò–∑–≤–ª–µ–∫–∞–µ—Ç stages, tools, indicators, rules –∏–∑ books —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GigaChat + Qwen3-Max
"""

import json
import logging
from typing import Dict, List, Any, Optional
from pathlib import Path

# GigaChat SDK
try:
    from gigachat import GigaChat
    GIGACHAT_AVAILABLE = True
except ImportError:
    GIGACHAT_AVAILABLE = False
    print("‚ö†Ô∏è GigaChat SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: pip install gigachat")

# Requesty AI –¥–ª—è fallback
from requesty_ai import RequestyClient

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class OutlineBuilder:
    """
    Agent B: –°—Ç—Ä–æ–∏—Ç outline.yaml –∏–∑ blocks.jsonl
    
    –°—Ç—Ä–∞—Ç–µ–≥–∏—è –º–æ–¥–µ–ª–µ–π:
    ü•á PRIMARY: GigaChat (–±–µ—Å–ø–ª–∞—Ç–Ω–æ, –±—ã—Å—Ç—Ä–æ 1.06s, –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç diagnostic)
    ü•à FALLBACK: Qwen3-Max —á–µ—Ä–µ–∑ Requesty AI (–ø—Ä–∞–≤–∏–ª—å–Ω–æ, —Ä—É—Å—Å–∫–∏–µ –∫–ª—é—á–∏)
    """
    
    def __init__(
        self,
        gigachat_credentials: Optional[str] = None,
        requesty_api_key: Optional[str] = None,
        use_gigachat: bool = True
    ):
        """
        Args:
            gigachat_credentials: API –∫–ª—é—á GigaChat (–µ—Å–ª–∏ None - –∏–∑ .env)
            requesty_api_key: API –∫–ª—é—á Requesty AI (–µ—Å–ª–∏ None - –∏–∑ .env)
            use_gigachat: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GigaChat –∫–∞–∫ primary (True) –∏–ª–∏ —Ç–æ–ª—å–∫–æ Qwen3-Max (False)
        """
        self.use_gigachat = use_gigachat and GIGACHAT_AVAILABLE
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GigaChat
        if self.use_gigachat:
            try:
                self.gigachat = GigaChat(
                    credentials=gigachat_credentials,
                    scope="GIGACHAT_API_PERS",
                    verify_ssl_certs=False
                )
                logger.info("‚úÖ GigaChat –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (PRIMARY)")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è GigaChat –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                self.gigachat = None
                self.use_gigachat = False
        else:
            self.gigachat = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Requesty AI (fallback)
        self.requesty = RequestyClient(api_key=requesty_api_key)
        logger.info("‚úÖ Requesty AI –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (FALLBACK)")
    
    
    def chat(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.3
    ) -> str:
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback
        
        Args:
            prompt: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º—Ç
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (0-1)
        
        Returns:
            –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
        """
        # –ü–æ–ø—ã—Ç–∫–∞ 1: GigaChat (PRIMARY)
        if self.use_gigachat and self.gigachat:
            try:
                logger.info("üá∑üá∫ –ó–∞–ø—Ä–æ—Å –∫ GigaChat...")
                
                # GigaChat –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø—Ä–æ–º—Ç
                full_prompt = prompt
                if system_prompt:
                    full_prompt = f"{system_prompt}\n\n{prompt}"
                
                response = self.gigachat.chat(full_prompt)
                result = response.choices[0].message.content
                
                logger.info(f"‚úÖ GigaChat –æ—Ç–≤–µ—Ç–∏–ª ({len(result)} —Å–∏–º–≤–æ–ª–æ–≤)")
                return result
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è GigaChat error: {e}")
                logger.info("‚Ü™Ô∏è –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ Qwen3-Max...")
        
        # –ü–æ–ø—ã—Ç–∫–∞ 2: Qwen3-Max —á–µ—Ä–µ–∑ Requesty AI (FALLBACK)
        try:
            logger.info("üá®üá≥ –ó–∞–ø—Ä–æ—Å –∫ Qwen3-Max...")
            
            messages = []
            if system_prompt:
                messages.append({'role': 'system', 'content': system_prompt})
            messages.append({'role': 'user', 'content': prompt})
            
            response = self.requesty.chat(
                messages=messages,
                model='alibaba/qwen3-max',
                temperature=temperature
            )
            
            logger.info(f"‚úÖ Qwen3-Max –æ—Ç–≤–µ—Ç–∏–ª ({len(response)} —Å–∏–º–≤–æ–ª–æ–≤)")
            return response
            
        except Exception as e:
            logger.error(f"‚ùå –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã: {e}")
            raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –Ω–∏ –æ—Ç –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏")
    
    
    def extract_chapters_from_blocks(self, blocks_jsonl_path: Path) -> List[Dict[str, Any]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–ª–∞–≤—ã –∏–∑ blocks.jsonl
        
        –ï—Å–ª–∏ –µ—Å—Ç—å heading (level ‚â§ 2) - –≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø–æ –Ω–∏–º
        –ï—Å–ª–∏ –Ω–µ—Ç heading - —Ä–∞–∑–±–∏–≤–∞–µ—Ç –ø–æ CHUNK_SIZE –±–ª–æ–∫–æ–≤
        
        Args:
            blocks_jsonl_path: –ü—É—Ç—å –∫ blocks.jsonl
        
        Returns:
            List[Dict] —Å –≥–ª–∞–≤–∞–º–∏: [{title, blocks, pages}, ...]
        """
        CHUNK_SIZE = 50  # –ë–ª–æ–∫–æ–≤ –Ω–∞ "–≥–ª–∞–≤—É" –µ—Å–ª–∏ –Ω–µ—Ç headings
        
        chapters = []
        current_chapter = None
        blocks_buffer = []
        has_headings = False
        
        with open(blocks_jsonl_path, 'r', encoding='utf-8') as f:
            for line in f:
                block = json.loads(line)
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞—á–∞–ª–æ –Ω–æ–≤–æ–π –≥–ª–∞–≤—ã (heading level ‚â§ 2)
                if block['type'] == 'heading' and block.get('meta', {}).get('level', 3) <= 2:
                    has_headings = True
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é –≥–ª–∞–≤—É
                    if current_chapter:
                        chapters.append(current_chapter)
                    
                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é –≥–ª–∞–≤—É
                    current_chapter = {
                        'title': block['text'],
                        'blocks': [block],
                        'pages': [block.get('source', {}).get('page', 1)]
                    }
                
                # –î–æ–±–∞–≤–ª—è–µ–º –±–ª–æ–∫ –∫ —Ç–µ–∫—É—â–µ–π –≥–ª–∞–≤–µ
                elif current_chapter:
                    current_chapter['blocks'].append(block)
                    page = block.get('source', {}).get('page')
                    if page and page not in current_chapter['pages']:
                        current_chapter['pages'].append(page)
                
                # –ï—Å–ª–∏ –Ω–µ—Ç headings - —Å–æ–±–∏—Ä–∞–µ–º –≤ –±—É—Ñ–µ—Ä
                else:
                    blocks_buffer.append(block)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –≥–ª–∞–≤—É (–µ—Å–ª–∏ –±—ã–ª–∏ headings)
        if current_chapter:
            chapters.append(current_chapter)
        
        # –ï—Å–ª–∏ headings –Ω–µ –Ω–∞–π–¥–µ–Ω—ã - –¥–µ–ª–∏–º –ø–æ CHUNK_SIZE
        if not has_headings and blocks_buffer:
            logger.warning(f"‚ö†Ô∏è –ó–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –¥–µ–ª—é –Ω–∞ chunks –ø–æ {CHUNK_SIZE} –±–ª–æ–∫–æ–≤")
            
            for i in range(0, len(blocks_buffer), CHUNK_SIZE):
                chunk = blocks_buffer[i:i+CHUNK_SIZE]
                pages = list(set([b.get('source', {}).get('page', 1) for b in chunk]))
                
                chapters.append({
                    'title': f"Chunk {i//CHUNK_SIZE + 1} (–±–ª–æ–∫–∏ {i+1}-{i+len(chunk)})",
                    'blocks': chunk,
                    'pages': sorted(pages)
                })
        
        logger.info(f"üìö –ò–∑–≤–ª–µ—á–µ–Ω–æ –≥–ª–∞–≤/chunks: {len(chapters)}")
        return chapters
    
    
    def analyze_chapter(self, chapter: Dict[str, Any]) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–¥–Ω—É –≥–ª–∞–≤—É –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç stages, tools, indicators, rules
        
        Args:
            chapter: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –≥–ª–∞–≤—ã {title, blocks, pages}
        
        Returns:
            Dict —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        """
        # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –≥–ª–∞–≤—ã
        chapter_text = '\n\n'.join([
            block['text'] 
            for block in chapter['blocks'] 
            if block['type'] in ['heading', 'paragraph', 'list']
        ])
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É (—á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç—å context window)
        max_chars = 4000
        if len(chapter_text) > max_chars:
            chapter_text = chapter_text[:max_chars] + "\n\n[...—Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω...]"
        
        # System prompt
        system_prompt = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç-–º–µ—Ç–æ–¥–æ–ª–æ–≥ –ø–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–º—É –∞–Ω–∞–ª–∏–∑—É –∏ –±—É—Ö–≥–∞–ª—Ç–µ—Ä–∏–∏.
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ –∏ –∏–∑–≤–ª–µ–∫–∞–π –∫–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏."""
        
        # User prompt —Å –ø—Ä–∏–º–µ—Ä–æ–º
        user_prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≥–ª–∞–≤—É –∫–Ω–∏–≥–∏ –∏ –∏–∑–≤–ª–µ–∫–∏:

1. **Stages (—ç—Ç–∞–ø—ã –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏)**: —à–∞–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å
   –§–æ—Ä–º–∞—Ç: [{{"title": "–Ω–∞–∑–≤–∞–Ω–∏–µ", "description": "–æ–ø–∏—Å–∞–Ω–∏–µ", "order": 1}}]

2. **Tools (–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã)**: —Ç–∞–±–ª–∏—Ü—ã, —à–∞–±–ª–æ–Ω—ã, —á–µ–∫-–ª–∏—Å—Ç—ã
   –§–æ—Ä–º–∞—Ç: [{{"title": "–Ω–∞–∑–≤–∞–Ω–∏–µ", "type": "table|template|checklist", "description": "–æ–ø–∏—Å–∞–Ω–∏–µ"}}]

3. **Indicators (–ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏)**: –º–µ—Ç—Ä–∏–∫–∏, —Ñ–æ—Ä–º—É–ª—ã
   –§–æ—Ä–º–∞—Ç: [{{"name": "–Ω–∞–∑–≤–∞–Ω–∏–µ", "formula": "—Ñ–æ—Ä–º—É–ª–∞ –µ—Å–ª–∏ –µ—Å—Ç—å", "description": "–æ–ø–∏—Å–∞–Ω–∏–µ"}}]

4. **Rules (–ø—Ä–∞–≤–∏–ª–∞)**: —É—Å–ª–æ–≤–∏—è –∏ –¥–µ–π—Å—Ç–≤–∏—è
   –§–æ—Ä–º–∞—Ç: [{{"condition": "–∫–æ–≥–¥–∞", "action": "—á—Ç–æ –¥–µ–ª–∞—Ç—å", "severity": "high|medium|low"}}]

5. **Methodology type**: –æ–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏
   –í–∞—Ä–∏–∞–Ω—Ç—ã: diagnostic | planning | analysis | standard

–û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
  "methodology_type": "diagnostic|planning|analysis|standard",
  "stages": [...],
  "tools": [...],
  "indicators": [...],
  "rules": [...]
}}

**–ì–ª–∞–≤–∞:** {chapter['title']}

**–¢–µ–∫—Å—Ç:**
{chapter_text}
"""
        
        # –ó–∞–ø—Ä–æ—Å –∫ LLM
        response = self.chat(
            prompt=user_prompt,
            system_prompt=system_prompt,
            temperature=0.3
        )
        
        # –ü–∞—Ä—Å–∏–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –∏–∑ markdown –±–ª–æ–∫–∞ ```json ... ```
            if '```json' in response:
                json_start = response.index('```json') + 7
                json_end = response.index('```', json_start)
                json_str = response[json_start:json_end].strip()
            elif '```' in response:
                json_start = response.index('```') + 3
                json_end = response.index('```', json_start)
                json_str = response[json_start:json_end].strip()
            else:
                json_str = response.strip()
            
            result = json.loads(json_str)
            result['source_chapter'] = chapter['title']
            result['pages'] = chapter['pages']
            
            logger.info(f"‚úÖ –ì–ª–∞–≤–∞ '{chapter['title']}' –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON: {e}")
            logger.error(f"–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {response[:200]}...")
            return {
                'methodology_type': 'unknown',
                'stages': [],
                'tools': [],
                'indicators': [],
                'rules': [],
                'source_chapter': chapter['title'],
                'pages': chapter['pages'],
                'error': str(e)
            }
    
    
    def build_outline(self, blocks_jsonl_path: Path) -> Dict[str, Any]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥: —Å—Ç—Ä–æ–∏—Ç outline.yaml –∏–∑ blocks.jsonl
        
        Args:
            blocks_jsonl_path: –ü—É—Ç—å –∫ blocks.jsonl —Ñ–∞–π–ª—É
        
        Returns:
            Dict —Å –ø–æ–ª–Ω—ã–º outline (–≥–æ—Ç–æ–≤ –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ YAML)
        """
        logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É: {blocks_jsonl_path}")
        
        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º –≥–ª–∞–≤—ã
        chapters = self.extract_chapters_from_blocks(blocks_jsonl_path)
        
        # 2. Map: –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é –≥–ª–∞–≤—É
        chapter_analyses = []
        for i, chapter in enumerate(chapters, 1):
            logger.info(f"üìñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –≥–ª–∞–≤—É {i}/{len(chapters)}: {chapter['title'][:50]}...")
            analysis = self.analyze_chapter(chapter)
            chapter_analyses.append(analysis)
        
        # 3. Reduce: –°–æ–±–∏—Ä–∞–µ–º –≤ –µ–¥–∏–Ω—ã–π outline
        outline = self._reduce_analyses(chapter_analyses)
        
        # 4. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è (Quality Gate compliance)
        outline = self._normalize_and_validate(outline)
        
        logger.info("‚úÖ Outline –ø–æ—Å—Ç—Ä–æ–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        return outline
    
    
    def _reduce_analyses(self, analyses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –∞–Ω–∞–ª–∏–∑—ã –≥–ª–∞–≤ –≤ –µ–¥–∏–Ω—ã–π outline (reduce —Ñ–∞–∑–∞)
        
        Args:
            analyses: –°–ø–∏—Å–æ–∫ –∞–Ω–∞–ª–∏–∑–æ–≤ –≥–ª–∞–≤
        
        Returns:
            –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π outline
        """
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â–∏–π —Ç–∏–ø –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ (–±–µ—Ä–µ–º —Å–∞–º—ã–π —á–∞—Å—Ç—ã–π)
        methodology_types = [a.get('methodology_type', 'unknown') for a in analyses]
        methodology_type = max(set(methodology_types), key=methodology_types.count)
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        all_stages = []
        all_tools = []
        all_indicators = []
        all_rules = []
        
        for analysis in analyses:
            all_stages.extend(analysis.get('stages', []))
            all_tools.extend(analysis.get('tools', []))
            all_indicators.extend(analysis.get('indicators', []))
            all_rules.extend(analysis.get('rules', []))
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (–ø–æ title/name)
        unique_stages = self._deduplicate_by_key(all_stages, 'title')
        unique_tools = self._deduplicate_by_key(all_tools, 'title')
        unique_indicators = self._deduplicate_by_key(all_indicators, 'name')
        unique_rules = self._deduplicate_by_key(all_rules, 'condition')
        
        outline = {
            'metadata': {
                'agent': 'Agent B v1.0 (GigaChat + Qwen3-Max)',
                'model_used': 'gigachat' if self.use_gigachat else 'qwen3-max',
                'chapters_processed': len(analyses)
            },
            'classification': {
                'methodology_type': methodology_type
            },
            'structure': {
                'stages': unique_stages,
                'tools': unique_tools,
                'indicators': unique_indicators,
                'rules': unique_rules
            }
        }
        
        logger.info(f"üìä –ò—Ç–æ–≥–æ: {len(unique_stages)} stages, {len(unique_tools)} tools, "
                   f"{len(unique_indicators)} indicators, {len(unique_rules)} rules")
        
        return outline
    
    
    def _deduplicate_by_key(self, items: List[Dict], key: str) -> List[Dict]:
        """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –∫–ª—é—á—É —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π"""
        import re
        seen = set()
        unique = []
        for item in items:
            value = (item.get(key, '') or '').strip().lower()
            value = re.sub(r'\s+', ' ', value)  # normalize whitespace
            
            if value and value not in seen:
                seen.add(value)
                unique.append(item)
        return unique
    
    
    def _normalize_and_validate(self, outline: Dict[str, Any]) -> Dict[str, Any]:
        """
        –ü–æ—Å—Ç–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ outline: –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è + –≤–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ–¥ B_QUALITY_GATE
        
        –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:
        - –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è stages —Å placeholder titles –∏–ª–∏ –ø—É—Å—Ç—ã–º–∏ descriptions
        - –ü–µ—Ä–µ–Ω—É–º–µ—Ä–∞—Ü–∏—è stage.order (1..N)
        - –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è + –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è indicators —Å –ø—É—Å—Ç—ã–º–∏ descriptions
        - –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è formula ('' ‚Üí None)
        - –ú–∞–ø–ø–∏–Ω–≥ severity (high/medium ‚Üí critical/warning/info/low)
        """
        import re
        structure = outline.get('structure', {})
        
        # 1. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è stages (—É–¥–∞–ª—è–µ–º placeholder'—ã –∏ –ø—É—Å—Ç—ã–µ descriptions)
        stages = structure.get('stages', [])
        valid_stages = []
        for stage in stages:
            title = (stage.get('title') or '').strip()
            desc = (stage.get('description') or '').strip()
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º placeholder'—ã
            if title in ['–®–∞–≥ 1', '–®–∞–≥ 2', '–®–∞–≥ 3', '–®–∞–≥ 4', '–≠—Ç–∞–ø 1', '–≠—Ç–∞–ø 2']:
                logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω placeholder stage: {title}")
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ descriptions
            if len(desc) < 15:
                logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω stage —Å –∫–æ—Ä–æ—Ç–∫–∏–º description: {title}")
                continue
            
            valid_stages.append(stage)
        
        # 2. –ü–µ—Ä–µ–Ω—É–º–µ—Ä–∞—Ü–∏—è stages (1..N)
        for i, stage in enumerate(valid_stages, 1):
            stage['order'] = i
        
        # 3. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è indicators (—É–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ descriptions) + –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        indicators = structure.get('indicators', [])
        valid_indicators = []
        seen_names = set()
        
        for ind in indicators:
            desc = (ind.get('description') or '').strip()
            
            if len(desc) < 10:
                logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω indicator —Å –ø—É—Å—Ç—ã–º description: {ind.get('name')}")
                continue
            
            # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ normalized name
            name = (ind.get('name') or '').strip().lower()
            name = re.sub(r'\s+', ' ', name)
            
            if name in seen_names:
                logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç indicator: {ind.get('name')}")
                continue
            
            seen_names.add(name)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è formula: '' ‚Üí None
            if ind.get('formula') == '':
                ind['formula'] = None
            
            valid_indicators.append(ind)
        
        # 4. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è severity –≤ rules
        SEVERITY_MAP = {
            'high': 'critical',
            'medium': 'warning',
            'low': 'info'
        }
        
        rules = structure.get('rules', [])
        for rule in rules:
            sev = rule.get('severity', 'info')
            rule['severity'] = SEVERITY_MAP.get(sev, sev)
        
        # 5. –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        outline['structure'] = {
            'stages': valid_stages,
            'tools': structure.get('tools', []),
            'indicators': valid_indicators,
            'rules': rules
        }
        
        logger.info(f"‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: {len(valid_stages)} stages, {len(valid_indicators)} indicators")
        
        return outline
