"""
–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ Requesty AI

–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞—Ö, –º–æ–¥–µ–ª—è—Ö –∏ –∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞—Ö
"""

from dataclasses import dataclass
from typing import List, Optional


@dataclass
class ModelInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
    id: str
    provider: str
    name: str
    context_window: int
    cost_per_1m_input: float  # USD
    cost_per_1m_output: float  # USD
    description: str
    best_for: List[str]


# ============================================
# –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ Requesty AI (–†–ï–ê–õ–¨–ù–´–ï –∏–∑ dashboard)
# ============================================

AVAILABLE_MODELS = {
    # Alibaba Qwen3-Max - –ü–†–ê–í–ò–õ–¨–ù–û –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é! –û—Ç–ª–∏—á–Ω—ã–π —Ä—É—Å—Å–∫–∏–π
    "alibaba/qwen3-max": ModelInfo(
        id="alibaba/qwen3-max",
        provider="Alibaba",
        name="Qwen3-Max",
        context_window=32_000,
        cost_per_1m_input=0.0,  # —É—Ç–æ—á–Ω—è–µ—Ç—Å—è
        cost_per_1m_output=0.0,
        description="–ú–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å –æ—Ç Alibaba, –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç diagnostic",
        best_for=["russian", "methodology_classification", "structured_extraction"]
    ),
    
    # DeepSeek - –û–ß–ï–ù–¨ –î–ï–®–ï–í–û! –û—Ç–ª–∏—á–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ü–µ–Ω–∞/–∫–∞—á–µ—Å—Ç–≤–æ
    "deepseek/deepseek-chat": ModelInfo(
        id="deepseek/deepseek-chat",
        provider="DeepSeek",
        name="DeepSeek Chat",
        context_window=64_000,
        cost_per_1m_input=0.14,  # $0.14 per 1M tokens!
        cost_per_1m_output=0.28,
        description="–û—á–µ–Ω—å –¥–µ—à–µ–≤–∞—è –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –∫–∏—Ç–∞–π—Å–∫–∞—è –º–æ–¥–µ–ª—å",
        best_for=["cost_effective", "reasoning", "coding", "multilingual"]
    ),
    
    # Smart/Task - —É–º–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è
    "smart/task": ModelInfo(
        id="smart/task",
        provider="Smart",
        name="Smart Task Router",
        context_window=128_000,
        cost_per_1m_input=0.10,  # –û—á–µ–Ω—å –¥–µ—à–µ–≤–æ –±–ª–∞–≥–æ–¥–∞—Ä—è routing
        cost_per_1m_output=0.30,
        description="–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–¥–∞—á–∏",
        best_for=["auto_routing", "cost_effective", "versatile"]
    ),
    
    # OpenAI models
    "openai/gpt-4o": ModelInfo(
        id="openai/gpt-4o",
        provider="OpenAI",
        name="GPT-4o",
        context_window=128_000,
        cost_per_1m_input=2.50,
        cost_per_1m_output=10.00,
        description="–°–∞–º–∞—è –º–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å OpenAI",
        best_for=["reasoning", "complex_tasks", "coding", "multilingual"]
    ),
    
    "openai/gpt-5-mini": ModelInfo(
        id="openai/gpt-5-mini",
        provider="OpenAI",
        name="GPT-5 Mini",
        context_window=128_000,
        cost_per_1m_input=0.15,
        cost_per_1m_output=0.60,
        description="–ë—ã—Å—Ç—Ä–∞—è –∏ –¥–µ—à–µ–≤–∞—è –º–æ–¥–µ–ª—å OpenAI",
        best_for=["simple_tasks", "fast_responses", "cost_effective"]
    ),
    
    # Google Gemini models
    "google/gemini-2.5-flash": ModelInfo(
        id="google/gemini-2.5-flash",
        provider="Google",
        name="Gemini 2.5 Flash",
        context_window=1_000_000,
        cost_per_1m_input=0.075,
        cost_per_1m_output=0.30,
        description="–û—á–µ–Ω—å –¥–µ—à–µ–≤–∞—è –º–æ–¥–µ–ª—å —Å –æ–≥—Ä–æ–º–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º",
        best_for=["cost_effective", "long_documents", "fast_responses"]
    ),
    
    "google/gemini-2.5-pro": ModelInfo(
        id="google/gemini-2.5-pro",
        provider="Google",
        name="Gemini 2.5 Pro",
        context_window=2_000_000,
        cost_per_1m_input=1.25,
        cost_per_1m_output=5.00,
        description="–û–≥—Ä–æ–º–Ω—ã–π context window –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
        best_for=["long_documents", "video", "multimodal"]
    ),
    
    # Coding —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ (–í–ê–ñ–ù–û: —É –Ω–∞—Å —Ä–∞–±–æ—Ç–∞–µ—Ç coding/, –Ω–æ –ù–ï google/)
    "coding/gemini-2.5-pro": ModelInfo(
        id="coding/gemini-2.5-pro",
        provider="Coding",
        name="Gemini 2.5 Pro (Coding)",
        context_window=2_000_000,  # 2M tokens!
        cost_per_1m_input=1.25,
        cost_per_1m_output=5.00,
        description="Gemini —Å 2M context, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è –∫–æ–¥–∞ –∏ –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
        best_for=["coding", "long_documents", "debugging", "code_generation"]
    ),
    
    # XAI Grok
    "xai/grok-code-fast-1": ModelInfo(
        id="xai/grok-code-fast-1",
        provider="XAI",
        name="Grok Code Fast 1",
        context_window=128_000,
        cost_per_1m_input=0.50,
        cost_per_1m_output=1.50,
        description="–ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–¥–∞ –æ—Ç XAI",
        best_for=["coding", "fast_responses", "technical_writing"]
    ),
}


# ============================================
# –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –Ω–∞—à–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ (–§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–†–ê–¢–ï–ì–ò–Ø: GigaChat + Qwen3-Max)
# ============================================

RECOMMENDED_MODELS = {
    "agent_b_outline": [
        "gigachat",  # ü•á PRIMARY: –±–µ—Å–ø–ª–∞—Ç–Ω–æ, –±—ã—Å—Ç—Ä–æ (1.06s), –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç diagnostic
        "alibaba/qwen3-max",  # ü•à FALLBACK: –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç diagnostic, —Ä—É—Å—Å–∫–∏–µ –∫–ª—é—á–∏
        "deepseek/deepseek-chat",  # ü•â –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: –¥–µ—à–µ–≤–æ, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ (4.31s)
    ],
    
    "agent_b_complex": [
        "gigachat",  # ü•á PRIMARY: –æ—Ç–ª–∏—á–Ω–æ –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–π
        "alibaba/qwen3-max",  # ü•à FALLBACK: –º–æ—â–Ω–∞—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
        "openai/gpt-4o",  # ü•â –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –∫—Ä–∏—Ç–∏—á–Ω–æ –∏ GigaChat/Qwen –Ω–µ —Å–ø—Ä–∞–≤–∏–ª–∏—Å—å
    ],
    
    "agent_c_compiler": [
        "gigachat-lite",  # ü•á PRIMARY: –±–µ—Å–ø–ª–∞—Ç–Ω–æ, –±—ã—Å—Ç—Ä–æ –¥–ª—è —à–∞–±–ª–æ–Ω–æ–≤
        "alibaba/qwen3-max",  # ü•à FALLBACK: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∑–∞–º–µ–Ω–∞
        "deepseek/deepseek-chat",  # ü•â –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: –¥–µ—à–µ–≤–æ
    ],
    
    "agent_d_qa": [
        "gigachat-pro",  # ü•á PRIMARY: –±–µ—Å–ø–ª–∞—Ç–Ω–æ, —Ö–æ—Ä–æ—à–∏–π reasoning
        "alibaba/qwen3-max",  # ü•à FALLBACK: –Ω–∞–¥–µ–∂–Ω–∞—è –∑–∞–º–µ–Ω–∞
        "claude-3.5-sonnet",  # ü•â PREMIUM: —Ç–æ–ª—å–∫–æ –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –∫–Ω–∏–≥ (–ü–ë–£, –ú–°–§–û)
    ],
    
    "long_documents": [
        "coding/gemini-2.5-pro",  # üèÜ 2M tokens context! (—Ä–∞–±–æ—Ç–∞–µ—Ç)
        "google/gemini-2.5-flash",  # 1M tokens context, –æ—á–µ–Ω—å –¥–µ—à–µ–≤–æ
    ],
    
    "full_books": [
        "coding/gemini-2.5-pro",  # –ú–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ü–µ–ª—É—é –∫–Ω–∏–≥—É —Ü–µ–ª–∏–∫–æ–º!
    ],
}


def get_model_info(model_id: str) -> Optional[ModelInfo]:
    """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏"""
    return AVAILABLE_MODELS.get(model_id)


def list_models(provider: Optional[str] = None) -> List[ModelInfo]:
    """
    –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —Ñ–∏–ª—å—Ç—Ä –ø–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—É)
    
    Args:
        provider: –§–∏–ª—å—Ç—Ä –ø–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—É (OpenAI, Anthropic, Google)
        
    Returns:
        –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
    """
    models = list(AVAILABLE_MODELS.values())
    
    if provider:
        models = [m for m in models if m.provider.lower() == provider.lower()]
    
    return models


def estimate_cost(
    model_id: str,
    input_tokens: int,
    output_tokens: int
) -> Optional[float]:
    """
    –û—Ü–µ–Ω–∏—Ç—å —Å—Ç–æ–∏–º–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞
    
    Args:
        model_id: ID –º–æ–¥–µ–ª–∏
        input_tokens: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ input —Ç–æ–∫–µ–Ω–æ–≤
        output_tokens: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ output —Ç–æ–∫–µ–Ω–æ–≤
        
    Returns:
        –°—Ç–æ–∏–º–æ—Å—Ç—å –≤ USD
    """
    model = get_model_info(model_id)
    
    if not model:
        return None
    
    cost_input = (input_tokens / 1_000_000) * model.cost_per_1m_input
    cost_output = (output_tokens / 1_000_000) * model.cost_per_1m_output
    
    return cost_input + cost_output


if __name__ == "__main__":
    # –¢–µ—Å—Ç
    print("üìä Available Models:\n")
    
    for model_id, model in AVAILABLE_MODELS.items():
        print(f"ü§ñ {model.name} ({model.provider})")
        print(f"   ID: {model.id}")
        print(f"   Context: {model.context_window:,} tokens")
        print(f"   Cost: ${model.cost_per_1m_input:.2f}/${model.cost_per_1m_output:.2f} per 1M tokens")
        print(f"   Best for: {', '.join(model.best_for)}")
        print()
    
    # –ü—Ä–∏–º–µ—Ä –æ—Ü–µ–Ω–∫–∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏
    print("üí∞ Cost Estimation Example:")
    cost = estimate_cost("openai/gpt-4o-mini", input_tokens=10_000, output_tokens=5_000)
    print(f"   10K input + 5K output tokens = ${cost:.4f}")
    
    print("\nüéØ Recommended Models for Agent B:")
    for model_id in RECOMMENDED_MODELS["agent_b_outline"]:
        model = get_model_info(model_id)
        print(f"   - {model.name} ({model.provider})")
